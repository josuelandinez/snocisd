# Restricted Boltzmann machine for NOCI
# NOTE Only works for hidden variables = 0, 1
# Naming systems:
# params: parameters of RBM, usually with the shape (nparam, lparam)
# tvecs: the vector form of the Thouless matrices (flattened)
# tmats: Thouless matrices
# rmats: rotation matrices of the MO coefficients (adding I on top of tmats)
# sdets: MO coefficients of the Slater determinants

import numpy as np
from scipy.optimize import minimize
import slater, noci
import itertools
import optax
import jax
import jax.numpy as jnp
from jax.config import config
config.update("jax_debug_nans", True)


def expand_vecs(params, hiddens=[0,1]):
    '''
    Expand the RBM vectors with respect to hiddens.
    Args:
        params: 2D array, RBM vectors of size (nparam, lparam)
    Kwargs:
        hiddens: list, hidden variables of RBM
    Returns:
        2D array: expanded Thouless vectors of size (nparam**nhidden, lparam).
    '''
    nparam = len(params)
    if nparam == 0:
        return []
    params = np.asarray(params)
    tvecs = []

    for iter in itertools.product(hiddens, repeat=nparam):
        sum_coeff = np.asarray(iter)
        t = np.dot(sum_coeff, params)
        tvecs.append(t)

    return tvecs


def add_vec(param0, tvecs):
    '''
    Given an RBM vector and the list of expanded Thouless vectors,
    return all new Thouless rotations generated by adding param0.
    Returns:
        The new set of vectors added to the old ones.
    NOTE: only for hiddens = [0, 1]
    '''
    tvecs_n = jnp.asarray(tvecs) + param0
    return tvecs_n

def tvecs_to_rotations(tvecs, tshape, normalize=True):
    '''
    Turn a vector to a rotation matrix.
    '''
    lt = len(tvecs)
    nvir, nocc = tshape
    rmats = []
    for i in range(lt):
        t = tvecs[i].reshape(2, nvir, nocc)
        r = slater.thouless_to_rotation(t) # put the identity operator on top
        if normalize:
            r = slater.normalize_rotmat(r)
        rmats.append(r)
    rmats = jnp.array(rmats)
    return rmats


def params_to_rotations(rbm_vecs, tshape, hiddens=[0,1], normalize=True):
    '''
    Turn RBM parameters into rotation matrices.
    Args:
        rbm_vecs: 2D array of size (nvecs, 2 x Nocc x Nvir), neural network weights.
        tshape: shape of the Thouless matrix (nvir, nocc)
    Kwargs:
        hiddens: hidden variables.
    Returns:
        A 3D numpy array of size (l^d, Norb, Nocc), where l is the length of hidden variables.
    '''
    nvecs = len(rbm_vecs)
    if nvecs == 0:
        return []
    rbm_vecs = jnp.asarray(rbm_vecs)
    nvir, nocc = tshape
    rmats = []

    #TODO use jnp.meshgrid to avoid loop
    for iter in itertools.product(hiddens, repeat=nvecs):
        sum_coeff = jnp.asarray(iter)
        t = jnp.dot(sum_coeff, rbm_vecs).reshape(2, nvir, nocc)
        r = slater.thouless_to_rotation(t) # put the identity operator on top
        if normalize:
            r = slater.normalize_rotmat(r)
        rmats.append(r)
    rmats = jnp.array(rmats)
    return rmats

def params_to_rmats(vecs, nvir, nocc, coeffs, normalize=False):

    vecs_all = jnp.dot(coeffs, vecs)
    vecs_all = vecs_all.reshape(-1, nvir, nocc)
    nvecs = vecs_all.shape[0]
    I = jnp.eye(nocc)
    Imats = jnp.tile(I, (nvecs)).T.reshape(nvecs, nocc, nocc) # 2 for spins
    rmats = jnp.concatenate([Imats, vecs_all], axis=1)
    rmats = rmats.reshape(-1, 2, nvir+nocc, nocc)
    return rmats

def expand_hiddens(hiddens, nvecs):
    '''
    Generate all possible combinations of the nvecs of hidden variables.
    Args:
        nvecs: number of RBM vectors
        hiddens: values of hidden variables
    Returns:
        2D array.
    '''
    coeffs = []
    for iter in itertools.product(hiddens, repeat=nvecs):
        sum_coeff = np.asarray(iter)
        coeffs.append(sum_coeff)
      
    coeffs = np.array(coeffs)
   
    return coeffs

def rbm_all(h1e, h2e, mo_coeff, nocc, nvecs,
            init_params=None, ao_ovlp=None, hiddens=[0,1],
            tol=1e-6, MaxIter=100, disp=False, method="BFGS"):
    '''
    Optimize the RBM parameters all together.
    Args:
        h1e: 2D array, one-body Hamiltonian
        h2e: 4D array, two-body Hamiltonian
        nocc: int, number of occupied orbitals
        nvecs: int, number of rbm_vectors
    kwargs:
        init_params: a list of vectors, initial guess of the RBM parameters.
        ao_ovlp: 2D array, overlap matrix among atomic orbitals
        hiddens: hidden variables for RBM neural network.
    NOTE: hard to converge when optimizing all.
    '''

    mo_coeff = jnp.array(mo_coeff)
    h1e = jnp.array(h1e)
    h2e = jnp.array(h2e)
    if ao_ovlp is not None:
        ao_ovlp = jnp.array(ao_ovlp)

    norb = h1e.shape[-1]
    nvir = norb - nocc
    lt = 2*nvir*nocc # 2 for spins

    # get expansion coefficients
    coeff_hidden = expand_hiddens(hiddens, nvecs)
    coeff_hidden = jnp.array(coeff_hidden)

    if init_params is None:
        init_params = jnp.random.rand(nvecs, lt)

    init_params = init_params.flatten(order='C')

    # get combination coefficients


    def cost_func(w):
        w_n = w.reshape(nvecs, -1)
        rmats = params_to_rmats(w_n, nvir, nocc, coeff_hidden, normalize=False)
        e = rbm_energy_nograd(rmats, mo_coeff, h1e, h2e, ao_ovlp=ao_ovlp)
        return e

    def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:

        opt_state = optimizer.init(params)

        @jax.jit
        def step(params, opt_state):
            loss_value, grads = jax.value_and_grad(cost_func)(params)
            updates, opt_state = optimizer.update(grads, opt_state, params)
            params = optax.apply_updates(params, updates)
            return params, opt_state, loss_value

        loss_last = 0
        for i in range(MaxIter):
            params, opt_state, loss_value = step(params, opt_state)
            dloss = loss_value - loss_last

            if i > 1000 and abs(dloss) < tol:
                print(f"Optimization converged after {i+1} steps.")
                break
            else:
                loss_last = loss_value
            if i%100 == 0:
                print(f'step {i}, loss: {loss_value};')

        return loss_value, params

    # NOTE: schecule doesn't do too well
    # Schedule learning rate
    # schedule = optax.warmup_cosine_decay_schedule(
    # init_value=1e-2,
    # peak_value=1.0,
    # warmup_steps=20,
    # decay_steps=4000,
    # end_value=1e-3,
    # )

    # optimizer = optax.chain(
    # optax.clip(1.0),
    # optax.adamw(learning_rate=schedule),
    # )

    optimizer = optax.adam(learning_rate=2e-2)
    energy, vecs = fit(init_params, optimizer)


    # params = minimize(cost_func, init_params, method=method, tol=tol, options={"maxiter":MaxIter, "disp": disp}).x
    # final_energy = cost_func(params)

    return energy, vecs


def rbm_fed(h1e, h2e, mo_coeff, nocc, nvecs,
            init_rbms=None, ao_ovlp=None, hiddens=[0,1],
            nsweep=3, tol=1e-7, MaxIter=100, disp=False, method="BFGS"):
    '''
    Kwargs:
        nsweep: maximum number of sweeps
    Optimize the RBM parameters one by one.
    '''
    mo_coeff = jnp.array(mo_coeff)
    h1e = jnp.array(h1e)
    h2e = jnp.array(h2e)
    if ao_ovlp is not None:
        ao_ovlp = jnp.array(ao_ovlp)

    norb = h1e.shape[-1]
    nvir = norb - nocc
    tshape = (nvir, nocc)

    if init_rbms is None:
        init_rbms = jnp.random.rand(nvecs, 2*nvir*nocc) # 2 for spins

    rot0_u = np.zeros((nvir+nocc, nocc))
    rot0_u[:nocc, :nocc] = np.eye(nocc)
    rot_hf = np.array([rot0_u, rot0_u]) # the HF state
    E0 = noci.noci_energy([rot_hf], mo_coeff, h1e, h2e, ao_ovlp=ao_ovlp, include_hf=True)
    e_hf = E0

    opt_rbms = [] # optimized RBM vectors
    opt_tvecs = jnp.array([np.zeros(2*nvir*nocc)]) # All Thouless vectors

    rmats = tvecs_to_rotations(opt_tvecs, tshape, normalize=True)
    sdets = slater.gen_determinants(mo_coeff, rmats)

    hmat = noci.full_hamilt_w_sdets(sdets, h1e, h2e, ao_ovlp=ao_ovlp)
    smat = noci.full_ovlp_w_rotmat(rmats)


    print("Start RBM FED...")
    for iter in range(nvecs):
        print(f"*****Optimizing Determinant {iter+1}*****")
        w0 = init_rbms[iter]
        e, w = opt_one_rbmvec(w0, opt_tvecs, h1e, h2e, mo_coeff, tshape,
                              ao_ovlp=ao_ovlp, hmat=None, smat=None,
                              tol=tol, MaxIter=MaxIter, disp=disp, method=method)
        #TODO keep on debugging

        opt_rbms.append(w)
        de = e - E0
        E0 = e
        print(f"##### Done optimizing determinant {iter+1}, energy lowered {de} #####")
        new_tvecs = add_vec(w, opt_tvecs) # new Thouless vectors from adding this RBM vector
        opt_tvecs = jnp.vstack([opt_tvecs, new_tvecs])
        # update hmat and smat
        lv = 2**iter
        h_n = jnp.zeros((lv*2, lv*2))
        s_n = jnp.zeros((lv*2, lv*2))
        h_n = h_n.at[:lv, :lv].set(hmat)
        s_n = s_n.at[:lv, :lv].set(smat)
        rmats_n = tvecs_to_rotations(new_tvecs, tshape, normalize=True)
        sdets_n = slater.gen_determinants(mo_coeff, rmats_n)
        hmat, smat = _expand_hs(h_n, s_n, rmats_n, sdets_n, rmats, sdets, tshape, h1e, h2e, mo_coeff, ao_ovlp=ao_ovlp)
        rmats = jnp.vstack([rmats, rmats_n])
        sdets = jnp.vstack([sdets, sdets_n])

    if nsweep > 0:
        if nvecs < 2:
            print("WARNING: No sweeps needed for only one determinant!")
        else:
            print("Start sweeping...")

            for isw in range(nsweep):
                E_s = E0
                print("Sweep {}".format(isw+1))
                for iter in range(nvecs):
                    # always pop the first vector and add the optimized to the end
                    w0 = opt_rbms.pop(0)
                    opt_vecs = expand_vecs(opt_rbms) # TODO not efficient
                    e, w = opt_one_rbmvec(w0, opt_vecs, h1e, h2e, mo_coeff, tshape,
                                        ao_ovlp=ao_ovlp, hmat=None, smat=None, tol=tol, MaxIter=MaxIter)
                    de = e - E0
                    E0 = e
                    print("Iter {}: energy lowered {}".format(iter+1, de))
                    opt_rbms.append(w)
                de_s = e - E_s
                print("***Energy lowered after Sweep {}: {}".format(isw+1, de_s))

    print("Total energy lowered: {}".format(e - e_hf))
    return e, opt_rbms

def opt_one_rbmvec(vec0, tvecs, h1e, h2e, mo_coeff, tshape, ao_ovlp=None,
                   hmat=None, smat=None, tol=1e-7, MaxIter=100, disp=False, method="BFGS"):
    '''
    Optimize one RBM vector with the other fixed.
    Args:
        vec0: 1D array, the RBM vector to be optimized.
        tvecs: a list of 1D arrays, previous Thouless vectors.

    Returns:
        float: energy
        1D array: optimized RBM vector.
    '''

    nvecs = len(tvecs)
    rmats = tvecs_to_rotations(tvecs, tshape, normalize=True)
    sdets = slater.gen_determinants(mo_coeff, rmats)

    if hmat is None: # construct previous Hamiltonian matrix
        hmat = noci.full_hamilt_w_sdets(sdets, h1e, h2e, ao_ovlp=ao_ovlp)
    if smat is None: # construct previous overlap matrix
        smat = noci.full_ovlp_w_rotmat(rmats)

    h_n = jnp.zeros((nvecs*2, nvecs*2))
    s_n = jnp.zeros((nvecs*2, nvecs*2))
    h_n = h_n.at[:nvecs, :nvecs].set(jnp.copy(hmat))
    s_n = s_n.at[:nvecs, :nvecs].set(jnp.copy(smat))

    tvecs = jnp.array(tvecs)
    def cost_func(w):
        tvecs_n = add_vec(w, tvecs) # newly added Thouless vectors
        rmats_n = tvecs_to_rotations(tvecs_n, tshape, normalize=True)
        sdets_n = slater.gen_determinants(mo_coeff, rmats_n)
        hm, sm = _expand_hs(h_n, s_n, rmats_n, sdets_n, rmats, sdets, tshape, h1e, h2e, mo_coeff, ao_ovlp=ao_ovlp)
        e = noci.solve_lc_coeffs(hm, sm)

        return e

    init_params = jnp.array(vec0)

    def fit(params: optax.Params, optimizer: optax.GradientTransformation) -> optax.Params:

        opt_state = optimizer.init(params)

        @jax.jit
        def step(params, opt_state):
            loss_value, grads = jax.value_and_grad(cost_func)(params)
            updates, opt_state = optimizer.update(grads, opt_state, params)
            params = optax.apply_updates(params, updates)
            return params, opt_state, loss_value

        loss_last = 0
        for i in range(MaxIter):
            params, opt_state, loss_value = step(params, opt_state)
            dloss = loss_value - loss_last

            if i > 1000 and abs(dloss) < tol:
                print(f"Optimization converged after {i+1} steps.")
                break
            else:
                loss_last = loss_value
            if i%500 == 0:
                print(f'step {i}, loss: {loss_value};')

        return loss_value, params

    # NOTE: schecule doesn't do too well
    # Schedule learning rate
    # schedule = optax.warmup_cosine_decay_schedule(
    # init_value=1e-2,
    # peak_value=1.0,
    # warmup_steps=20,
    # decay_steps=4000,
    # end_value=1e-3,
    # )

    # optimizer = optax.chain(
    # optax.clip(1.0),
    # optax.adamw(learning_rate=schedule),
    # )

    optimizer = optax.adam(learning_rate=2e-2)
    energy, vec = fit(init_params, optimizer)

    #v = minimize(cost_func, vec0, method=method, tol=tol, options={"maxiter":MaxIter, "disp": disp}).x
    #energy = cost_func(v)

    return energy, vec

def _expand_hs(h_n, s_n, rmats_n, sdets_n, rmats, sdets, tshape, h1e, h2e, mo_coeff, ao_ovlp=None):
    '''
    Expand the
    '''
    nvecs = len(rmats_n)
    hm = jnp.copy(h_n)
    sm = jnp.copy(s_n)
    # TODO avoid the following
    hm = hm.at[nvecs:, nvecs:].set(noci.full_hamilt_w_sdets(sdets_n, h1e, h2e, ao_ovlp=ao_ovlp))
    sm = sm.at[nvecs:, nvecs:].set(noci.full_ovlp_w_rotmat(rmats_n))

    # crossing terms
    for i in range(nvecs): # old vectors
        for j in range(nvecs): # new vectors
            _s = slater.ovlp_rotmat(rmats[i], rmats_n[j])
            _h = slater.trans_hamilt(sdets[i], sdets_n[j], h1e, h2e, ao_ovlp=ao_ovlp)
            sm = sm.at[i, nvecs+j].set(_s)
            sm = sm.at[nvecs+j, i].set(_s)
            hm = hm.at[i, nvecs+j].set(_h)
            hm = hm.at[nvecs+j, i].set(_h)

    return hm, sm


def energy_rbm(rbmvecs, mo_coeff, h1e, h2e, tshape, ao_ovlp=None, hiddens=[0,1]):

    rmats = params_to_rotations(rbmvecs, tshape, hiddens=hiddens, normalize=True)
    e = noci.noci_energy(rmats, mo_coeff, h1e, h2e, ao_ovlp=ao_ovlp, include_hf=True)
    return e

def rbm_energy_nograd(rmats, mo_coeff, h1e, h2e, ao_ovlp=None):

    # TODO rewrite this as a giant einsum

    nt = len(rmats)

    hmat = jnp.zeros((nt, nt))
    smat = jnp.zeros((nt, nt))

    for i in range(nt):
        for j in range(i+1):
            sdet1 = slater.rotation(mo_coeff, rmats[i])
            sdet2 = slater.rotation(mo_coeff, rmats[j])
            dm, ovlp = slater.make_trans_rdm1(sdet1, sdet2, ao_ovlp=ao_ovlp, return_ovlp=True)
            jk = slater.get_jk(h2e, dm)
            hval1 = jnp.einsum('ij, nji -> ', h1e, dm)
            hval2 = jnp.einsum('nij, nji -> ', jk, dm)
            hval = (hval1 + 0.5 * hval2) * ovlp
            hmat = hmat.at[i, j].set(hval)
            hmat = hmat.at[j, i].set(hval.conj()) 
            smat = smat.at[i, j].set(ovlp)
            smat = smat.at[j, i].set(ovlp.conj())

    energy = noci.solve_lc_coeffs(hmat, smat, return_vec=False)
    return energy

if __name__ == "__main__":
    print("Main function:\n")
    # params = np.random.rand(3,4)
    # params_to_rotations(params)
